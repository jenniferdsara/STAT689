{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Support Vector Regression in R: Investigating outlier-sensitivity in Machine Learning Models\n",
    "##  Jennifer Dargin and Huan Luo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we learned that the least squares (LS) based regression model (e.g., ridge regression) is sensitive to datasets contaminated with outliers, which can lead to biased predictions of the fitted regression model. Machine learning (ML) techniques, as advanced statistical learning methods, have achieved great success in many disciplinaries. The purpose of this project is to determine whether or not the fit of ML methods, particularly an LS-based method, will be affected by the presence of outliers in a dataset. If so, to what extent will they be negatively affected compared to ridge regression? In this project, we would like to investigate how outliers affect the LS based ML models, explore the levels of negative interferences affected by outliers between LS based ML models and ridge regression, and validate the performance of existing solutions to reduce the negative affect of outliers for LS based ML techniques and ridge regression. Specifically, the main contributions in this project is organized as following:\n",
    "\n",
    "1. We investigated a well-known LS based ML method, called least squares support vector regression (LS-SVR), which was originally developed by Suykens et al. (2002). The prediction performance between LS-SVR and ridge regression for a simulated dataset excluding the outliers is first compared to illustrate their original prediction capabilities (i.e., prediction is not negatively affected by outliers).\n",
    "\n",
    "2. We randomly added the outliers to the simulated dataset. The size of outliers we added is equal to 10 percent of the total number of the simulated dataset. Then we re-fited these two models to the simulated dataset contaminated by the outliers, and the prediction performance of these two models for the contaminated dataset is compared as well as the comparision with those on the simulated dataset excluding outliers. Through this comparison, the levels of negative interferences affected by outliers between LS-SVR and ridge regression can be identified.\n",
    "\n",
    "3. Two existing solutions to reduce the negative affect of outliers for LS-SVR and ridge regression were studied. The one for LS-SVR is called weighted LS-SVR, which is originally developed by Suykens et al. (2002), and another for ridge regression is called weighted LS, which is a well-known approach. The prediction performance of these two weighted models for the contaminated simulated dataset is compared and the performance comparison between these weighted and unweighted models is also conducted such that the effectiveness of these two existing solutions can be validated.\n",
    "\n",
    "4. All of the codes regarding the developments of LS-SVR, ridge regression, weighted LS-SVR, and weighted ridge regression are developed from scratch using R programming language by the authors. No packages were used in this project. The detailed programming procedures are also presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theory of LS-SVR and Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Theory of LS-SVR\n",
    "\n",
    "The detailed information about the formulation of LS-SVR can be found in Suykens et al. (2002). Here, we briefly summarize the formulation and present the mathmatical equations necessary for the code developement, which can be found as follows.\n",
    "\n",
    "Given the training set $\\{(x_i,y_i)\\}_{i=1}^n$, where $x_i \\in R^{p}$ and $y_i \\in R$, the objective of the LS-SVR is to find $w \\in R^{h}$ and $b$ to minimize the following objective function:\n",
    "\n",
    "\\begin{align*}\n",
    "min J(w,e) = \\frac{1}{2}w^Tw + \\frac{1}{2}\\gamma\\sum_{i=1}^ne_{i}^2                        \n",
    "\\end{align*}           \n",
    "\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$y_i = w^T\\varphi(x_i) + b + e_i, i = 1,...,n$$\n",
    "\n",
    "Where $\\varphi(\\centerdot): R^p \\rightarrow R^h$ is a function which maps the input space to a higher dimensional feature space; $e_i \\in R$ is error variable; $\\gamma \\in R$ is regularization parameter; $b$ is bias term.  \n",
    "\n",
    "The Lagrangian function for the equations above can be established as follows:\n",
    "\n",
    "$$L(w,b,e;\\alpha) = J(w,e) - \\sum_{i=1}^n\\alpha_i\\{w^T\\varphi(x_i)+b+e_i-y_i\\}$$\n",
    "\n",
    "Where $\\alpha_i \\in R$ (also called support values) is Lagrange multiplier. The Karush-Kuhn-Tucker(KKT) conditions for optimality are given by:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\frac{\\partial L}{\\partial w}=0 \\rightarrow w = \\sum_{i=1}^n\\alpha_i\\varphi(x_i)\\\\\n",
    "&\\frac{\\partial L}{\\partial b}=0 \\rightarrow \\sum_{i=1}^n\\alpha_i=0\\\\\n",
    "&\\frac{\\partial L}{\\partial e_i}=0 \\rightarrow \\alpha_i = \\gamma e_i, i = 1,...,n\\\\\n",
    "&\\frac{\\partial L}{\\partial \\alpha_i}=0 \\rightarrow w^T\\varphi(x_i)+b+e_i-y_i=0, i = 1,...,n\n",
    "\\end{align*}\n",
    "\n",
    "After elimination of $w,e$, one can obtain the following matrix equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "0 & 1_{1xn}\\\\\n",
    "1_{nx1} & K_{nxn}+\\frac{1}{\\gamma}I_{nxn}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "b\\\\\n",
    "\\alpha_{nx1}\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "0\\\\\n",
    "y_{nx1}\n",
    "\\end{array}\n",
    "\\right ]\n",
    "\\end{equation}\n",
    "\n",
    "Where $K_{nxn}(x_i,x_j)=\\varphi(x_i)^T \\varphi(x_j), i,j=1,...,n$ is a kernel function that meets the Mercer's condition.\n",
    "\n",
    "After solving $\\alpha_{nx1}$ and $b$, the resulting LS-SVR model for prediction can be obtained as follows:\n",
    "\n",
    "$$ y(x) = \\sum_{i=1}^n \\alpha_iK(x,x_i)+b$$\n",
    "\n",
    "In this project, we use RBF kernel, which is defined as follows:\n",
    "\n",
    "$$K(x_i,x_j)=e^{-\\frac{||x_i - x_j||_{2}^2}{\\sigma^2}}$$\n",
    "\n",
    "It is observed that training of LS-SVR requires the two hyper-parameters, one is the regularization parameter $\\gamma$ and another is the RBF kernel parameter $\\sigma^2$, to be apropriately selected, as these two hyper-parameters can significantly affect the prediction performance of LS-SVR given a training set $\\{(x_i,y_i)\\}_{i=1}^n$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Tuning the hyper-parameters\n",
    "\n",
    "There are many optimization methods that can be used to tune these two hyper-parameters (e.g., genetic algorithm, particle swarm algorithm, simulated annealing). In this project, we use an exhaustive serach optimization algorithm, called grid search, to tune these two hyper-parameters. We use this global optimization algorithm rather than others aforementioned is because other global optimization algorithms typically include a stochastic process and are sensitive to the initial values. On the contrary, grid search algorithm can guarantee that the optimum hyper-parameters can be achieved as long as the parameter space is apropriately defined. The parameter space we set is $\\{e^{-10:2:10}\\}$ for these two hyper-parameters.\n",
    "\n",
    "After the parameter space is defined, the next step is to creat a cost function that is related to the prediction performance of the LS-SVR. The cost function we use is mean squared error (MSE) obtained based on 10-fold cross validation, such that the optimum LS-SVR will not be overfitting. Given the training set $\\{(x_i,y_i)\\}_{i=1}^n$, the detailed hyper-parameters tuning procedure using grid search approach is summarized below:\n",
    "\n",
    "1. Set parameter space $\\Theta$ for regularization and RBF kernel parameters\n",
    "\n",
    "2. For each pair of $\\{(\\gamma,\\sigma^2)_j\\}_{j=1}^{length(\\Theta)}$, obtain MSE by LS-SVR procedure based on the 10-fold cross validation\n",
    "\n",
    "3. Seclect the optimum pair $(\\gamma,\\sigma^2)_o$ that make the MSE minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Theory of Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge regression is a well-known approach and is an extension of ordinary least squares method by adding a regularization parameter $\\lambda$ to penalize the regression coefficients for avoiding the overfitting. The detailed formulation of ridge regression can be easily found online. Thus, here we briefly summarize the mathmatical equations necessary for code development.\n",
    "\n",
    "Given the training set $\\{(x_i,y_i)\\}_{i=1}^n$, where $x_i \\in R^{p}$ and $y_i \\in R$, the objective of ridge regression is to find regression coefficients $\\beta \\in R^{p+1}$ that minimize the following objective function\n",
    "\n",
    "\\begin{align*}\n",
    "min J(\\beta) = \\sum_{i=1}^ne_{i}^2 + \\lambda \\sum_{j=0}^p\\beta_{j}^2                    \n",
    "\\end{align*}  \n",
    "\n",
    "subject to:\n",
    "\n",
    "$$y_i = \\sum_{j=0}^p\\beta_jx_{ij} + e_i, i = 1,...,n$$\n",
    "\n",
    "where $x_{i0} = 1$, $\\lambda$ is a regularization parameter.\n",
    "\n",
    "The regression coefficient $\\beta$ can be obtained using the following equation\n",
    "\n",
    "$$\\beta = (X^TX + \\lambda I)^{-1}X^Ty$$\n",
    "\n",
    "where $X \\in R^{n\\times(p+1)}$, $I \\in R^{(p+1)\\times(p+1)}$ is identity matrix, and $y \\in R^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Development of R Code for Implementation of LS-SVR and Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Development of R code for Implementation of LS-SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing the LS-SVR model using R programming language, five functions, called $kernelmatrix$, $trainlssvr$, $kcrossvalidation$,$tunelssvr$, and $predictlssvr$ are developed. Each of the five functions is explained in detail to let the readers easily understand how they are coded. The detailed information is presented as follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Development of R code for Implementation of $kernelmatrix$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF kernel is calculated using the following equation:\n",
    "\n",
    "$$K(x_i,x_j)=e^{-\\frac{||x_i - x_j||_{2}^2}{\\sigma^2}}$$\n",
    "\n",
    "The kernel matrix can be written as the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "K(x_1,x_1) & \\cdots & K(x_1,x_n)\\\\\n",
    "\\vdots & \\ddots &\\vdots\\\\\n",
    "K(x_n,x_1) & \\cdots & K(x_n,x_n)\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "K_{nxn}(x_i,x_j); i,j=1,...,n\n",
    "\\end{equation}\n",
    "\n",
    "To implement the RBF kernel matrix in computer, the first step is to make sure what is the entry in the kernel matrix. According to the RBF kernel equation, the components in the RBF kernel can be decomposed as two parts, one is for $||x_i - x_j||_{2}^2$ and another is for RBF kernel parameter $\\sigma^2$. As the RBF kernel parameter $\\sigma^2$ keeps the same value across all the entries in the RBF kernel matrix, it is more straightforward to first finish the former part and then finish the latter part. \n",
    "\n",
    "The most easy way to implement the RBF kernel is to use for loops. However, in class we learned that this process will be computational expensive. Thus, in this project, we directly use the matrix operation instead of using for loops. The detailed procedure is given as follows:\n",
    "\n",
    "Given a dataset $\\{(x_i,y_i)\\}_{i=1}^n$, where $x_i \\in R^p$ and $y_i \\in R$, the matrix of the predictor variable in the dataset can be expressed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "x_{11} & \\cdots & x_{1p}\\\\\n",
    "\\vdots & \\ddots &\\vdots\\\\\n",
    "x_{n1} & \\cdots & x_{np}\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "x_1\\\\\n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\n",
    "\\right]\n",
    "= X\n",
    "\\end{equation}\n",
    "\n",
    "where $X \\in R^{n \\times p}$ is the matrix.\n",
    "\n",
    "As $||x_i - x_j||_{2}^2 = x_i^2 + x_j^2 - 2x_ix_j$, the former in the RBF kernel matrix can be written as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "x_1^2 + x_1^2 - 2x_1x_1 & \\cdots & x_1^2 + x_n^2 - 2x_1x_n\\\\\n",
    "\\vdots & \\ddots &\\vdots\\\\\n",
    "x_n^2 + x_1^2 - 2x_nx_1 & \\cdots & x_n^2 + x_n^2 - 2x_nx_n\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "x_1^2 & \\cdots & x_1^2\\\\\n",
    "\\vdots & \\ddots &\\vdots\\\\\n",
    "x_n^2 & \\cdots & x_n^2\n",
    "\\end{array}\n",
    "\\right]\n",
    "+\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "x_1^2 & \\cdots & x_n^2\\\\\n",
    "\\vdots & \\ddots &\\vdots\\\\\n",
    "x_1^2 & \\cdots & x_n^2\n",
    "\\end{array}\n",
    "\\right]\n",
    "-\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "2x_1x_1 & \\cdots & 2x_1x_n\\\\\n",
    "\\vdots & \\ddots &\\vdots\\\\\n",
    "2x_nx_1 & \\cdots & 2x_nx_n\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "As $x_i^2 = \\sum_{t=1}^px_{it}^2$, thus the matrix equations above can be expressed in the computer using R code command rowSums() as follows\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "x_1^2 & \\cdots & x_1^2\\\\\n",
    "\\vdots & \\ddots &\\vdots\\\\\n",
    "x_n^2 & \\cdots & x_n^2\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "as.matrix(rowSums(X^2)) \\times 1_{1 \\times n}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "2x_1x_1 & \\cdots & 2x_1x_n\\\\\n",
    "\\vdots & \\ddots &\\vdots\\\\\n",
    "2x_nx_1 & \\cdots & 2x_nx_n\n",
    "\\end{array}\n",
    "\\right]\n",
    "= 2XX^T\n",
    "\\end{equation}\n",
    "\n",
    "We denote $Xtr = as.matrix(rowSums(X^2)) \\times 1_{1 \\times n}$. So, the matrix operation can be expressed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "x_1^2 + x_1^2 - 2x_1x_1 & \\cdots & x_1^2 + x_n^2 - 2x_1x_n\\\\\n",
    "\\vdots & \\ddots &\\vdots\\\\\n",
    "x_n^2 + x_1^2 - 2x_nx_1 & \\cdots & x_n^2 + x_n^2 - 2x_nx_n\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "Xtr + Xtr^T - 2XX^T \n",
    "\\end{equation}\n",
    "\n",
    "We denote $K = Xtr + Xtr^T - 2XX^T$, so the kernel matrix in the training procedure can be coded as\n",
    "$$K_{nxn}(x_i,x_j) = e^{-\\frac{K}{\\sigma^2}}$$ \n",
    "\n",
    "The R code named $kernelmatrix$ function to represent the RBF kernel matrix is developed according to the mathmatics presented above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code for developing RBF kernelmatrix function\n",
    "kernelmatrix <- function(Xtrain,sigma2,Xtest){\n",
    "    \n",
    "  # This code is to develop a kernel matrix. The current kernel function is mainly\n",
    "  # for radial basis function (RBF) kernel. K(xi,xj) = exp(-sigma2||xi-xj||\n",
    "  Xtrain <- as.matrix(Xtrain) # make sure the input Xtrain is a matrix format\n",
    "    \n",
    "  if(missing(Xtest)){ # In the training procedure, the testing set is no need to input\n",
    "    n <- nrow(Xtrain) # number of observations in the training set (Xtrain,ytrain)\n",
    "    ones <- matrix(rep(1,n),nrow=1,ncol=n) # Construct the 1 vector with 1xn\n",
    "    Xtr <- as.matrix(rowSums(Xtrain^2))%*%ones # Calculate the Xtr\n",
    "    K <- Xtr + t(Xtr) - 2*(Xtrain%*%t(Xtrain)) # Calculate the K\n",
    "    return(exp(-(K / 2*sigma2))) # return the RBF kernel matrix\n",
    "  } else { # In the testing procedure, the testing set needs to be input, see the last equation in section 2.1\n",
    "    \n",
    "    # This procedure is similar to the steps introduced in section 3.1.1\n",
    "    Xtest <- as.matrix(Xtest)\n",
    "    ones_Xt <- matrix(rep(1,nrow(Xtest)),nrow=nrow(Xtest),ncol=1)\n",
    "    ones_Xtr <- matrix(rep(1,nrow(Xtrain)),nrow=1,ncol=nrow(Xtrain))  \n",
    "    Xtr <- ones_Xt%*%t(rowSums(Xtrain^2))\n",
    "    Xt <- as.matrix(rowSums(Xtest^2))%*%ones_Xtr\n",
    "    K <- Xt + Xtr - 2*(Xtest%*%t(Xtrain)) \n",
    "    return(exp(-(K / 2*sigma2)))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 Development of R code for Implementation of $trainlssvr$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $trainlssvr$ function is mainly to solve the linear system presented in the section 2.1 to calculate the $\\alpha_{n\\times1}$ and $b$. The linear system is also presented here for convenience.\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "0 & 1_{1xn}\\\\\n",
    "1_{nx1} & K_{nxn}+\\frac{1}{\\gamma}I_{nxn}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "b\\\\\n",
    "\\alpha_{nx1}\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "0\\\\\n",
    "y_{nx1}\n",
    "\\end{array}\n",
    "\\right ]\n",
    "\\end{equation}\n",
    "\n",
    "It is observed that the coeeficient matrix of the linear system is not a positive definite matrix. So, it will be difficult to directly solve this linear system when the coefficient matrix is close to singular. \n",
    "\n",
    "There are two methods that can effectively solve this problem. One is using the pseudoinverse of the coefficient matrix, and another is using the method proposed by Suykens et al. (2002). In this project, we implement the second one. The detailed procedure is summarized as following.\n",
    "\n",
    "1. rearranging the linear system as the following\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "s & 0_{1xn}\\\\\n",
    "0_{nx1} & H_{nxn}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "b\\\\\n",
    "\\alpha_{nx1}+bH_{nxn}^{-1}1_{nx1}\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "1_{1xn}H_{nxn}^{-1}y_{nx1}\\\\\n",
    "y_{nx1}\n",
    "\\end{array}\n",
    "\\right ]\n",
    "\\end{equation}\n",
    "\n",
    "where $H_{nxn} = K_{nxn}+\\frac{1}{\\gamma}I_{nxn}$ is a positive definite matrix, $s = 1_{1xn}H_{nxn}^{-1}1_{nx1}$. As $H_{nxn}$ is a positive definite matrix, which means $H_{nxn}^{-1}$ always exists and is also a positive definite matrix. This leads to $s = 1_{1xn}H_{nxn}^{-1}1_{nx1}>0$ always hold. Therefore, the coefficient matrix of the rearranged linear system above is also a positive definite matrix, which yields a unique solution to this linear system.\n",
    "\n",
    "2. solve $\\eta_{nx1}$ and $\\nu_{nx1}$ from $H_{nxn}\\eta_{nx1}=1_{nx1}$ and $H_{nxn}\\nu_{nx1}=y_{nx1}$.\n",
    "\n",
    "3. Calculate $s=1_{1xn}\\eta_{nx1}$\n",
    "\n",
    "4. calaculate the $b = eta_{nx1}^{T}y_{nx1}/s$ and $\\alpha_{nx1} = \\nu_{nx1} - b\\eta_{nx1}$.\n",
    "\n",
    "The R code named $trainlssvr$ function to solve the linear sysetem to calculate the $\\alpha$ and $b$ is developed according to the mathmatics presented above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code for developing trainlssvr function\n",
    "trainlssvr <- function(Xtrain,ytrain,gamma,sigma2){\n",
    "    Xtrain <- as.matrix(Xtrain)\n",
    "    ytrain <- as.matrix(ytrain)\n",
    "    n <- nrow(Xtrain)\n",
    "    Ktr <- kernelmatrix(Xtrain,sigma2) # Ktr represents the RBF kernel matrix\n",
    "    ones_col <- matrix(rep(1,n),nrow=n,1) # ones_col represents the 1 vector with nx1\n",
    "    ones_row <- matrix(rep(1,n),nrow=1,ncol=n) # ones_row represents the 1 vector with 1xn\n",
    "    Gamma <- (1/gamma)*diag(rep(1,n)) # Gamma represents the diagonal matrix (1/gamma)*I with nxn\n",
    "    H <- Ktr + Gamma # H represents the positive definite matrix K + (1/gamma)*I with nxn\n",
    "    eta <- solve(H,ones_col)\n",
    "    nu <- solve(H,ytrain)\n",
    "    s <- as.numeric(ones_row %*% eta)\n",
    "    b <- as.numeric(t(eta) %*% ytrain / s)\n",
    "    alpha <- nu - b*eta\n",
    "    result <- c(b,alpha)\n",
    "    return(result)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3 Development of R code for Implementation of $predictlssvr$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the $\\alpha_{n\\times1}$ and $b$, the $predictlssvr$ function is mainly to predict the response variable of testing set based on the equation below.\n",
    "\n",
    "$$ y(x) = \\sum_{i=1}^n \\alpha_iK(x,x_i)+b$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code for developing predictlssvr function\n",
    "predictlssvr <- function(Xtrain,ytrain,gamma,sigma2,alpha,b,Xtest){\n",
    "  Xtrain <- as.matrix(Xtrain)\n",
    "  ytrain <- as.matrix(ytrain)\n",
    "  Xtest <- as.matrix(Xtest)\n",
    "  alpha <- as.matrix(alpha)\n",
    "  Kt <- kernelmatrix(Xtrain,sigma2,Xtest) # Kt represents the RBF kernel matrix for the testing procedure\n",
    "  coeff <- rbind(b,alpha) # This is to predict multiple points in the testing set using matrix format\n",
    "  one <- rep(1,nrow(Xtest))\n",
    "  Kt <- cbind(one,Kt)\n",
    "  return(Kt%*%coeff)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.4 Development of R code for Implementation of $kcrossvalidation$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k fold cross validation is a well-known approach, which is used to avoid the overfitting in the training procedure. The theory of this approach can be found in https://en.wikipedia.org/wiki/Cross-validation_(statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code for developing k-cross validation used to tune hyper-parameters\n",
    "# Use k-fold cross validation \n",
    "kcrossvalidation <- function(Xtrain,ytrain,gamma,sigma2,k){\n",
    "    Xtrain <- as.matrix(Xtrain)\n",
    "    ytrain <- as.matrix(ytrain)\n",
    "    n <- nrow(Xtrain)\n",
    "    size <- floor(n/k) # calculate the size of data in each of k folds\n",
    "    costs <- matrix(0,nrow=k,ncol=1)\n",
    "    for(i in 1:k){\n",
    "        if(i==1){\n",
    "            train <- (size+1):n\n",
    "            validation <- ((i-1)*size+1):(i*size)\n",
    "        } else if(i==k){\n",
    "            train <- 1:((i-1)*size)\n",
    "            validation <- ((i-1)*size+1):n\n",
    "        } else {\n",
    "            train1 <- 1:((i-1)*size)\n",
    "            train2 <- (i*size+1):n\n",
    "            train <- c(train1,train2)\n",
    "            validation <- ((i-1)*size+1):(i*size)\n",
    "        }\n",
    "        \n",
    "        Xtr <- Xtrain[train,] # predictor variable in the training set in the cross validation procedure\n",
    "        ytr <- ytrain[train,] # response variable in the training set in the cross validation procedure\n",
    "        Xvalid <- Xtrain[validation,] # predictor variable in the validation set in the cross validation procedure\n",
    "        yvalid <- ytrain[validation,] # response variable in the validation set in the cross validation procedure\n",
    "        result <- trainlssvr(Xtr,ytr,gamma,sigma2)\n",
    "        b <- result[1]\n",
    "        alpha <- result[-1]\n",
    "        ypvalid <- predictlssvr(Xtr,ytr,gamma,sigma2,alpha,b,Xvalid)\n",
    "        mse <- mean((yvalid - ypvalid)^2) # Calculate the cost function, here is mean squared error(MSE)\n",
    "        costs[i] <- mse    # record the MSE obtained in each k fold cross validation procedure\n",
    "    }\n",
    "    cost = mean(costs) # calculate the average of the MSE in the whole k fold cross validation procedure\n",
    "    return(cost)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.5 Development of R code for Implementation of $tunelssvr$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $tunelssvr$ function is to tune the hyper-parameters, $\\gamma$ and $\\sigma^2$, to obtain the optimal $\\gamma$ and $\\sigma^2$, which makes the LS-SVR optimum. The hyper-parameter optimization procedure can be found in section 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code for developing tunelssvr function using exhaustively search optimiziation algorithm called grid search\n",
    "tunelssvr <- function(Xtrain,ytrain,gamma,sigma2,k){\n",
    "    Xtrain <- as.matrix(Xtrain)\n",
    "    ytrain <- as.matrix(ytrain)\n",
    "    numg <- length(gamma)\n",
    "    nums <- length(sigma2)\n",
    "    result <- matrix(0,nrow=numg*nums,ncol=3) # create a matrix named result to record the results\n",
    "    count <- 0\n",
    "    for(i in 1:numg){\n",
    "        for(j in 1:nums){\n",
    "            count <- count + 1\n",
    "            cost <- kcrossvalidation(Xtrain,ytrain,gamma[i],sigma2[j],k)\n",
    "            result[count,1] <- gamma[i] # the first column records the regularization parameter gamma \n",
    "            result[count,2] <- sigma2[j] # the second column records the RBF kernel parameter sigma2\n",
    "            result[count,3] <- cost # the third column records the estimated cost function (i.e., mse)\n",
    "        }\n",
    "    }\n",
    "    min_cost <- min(result[,3]) # find the minimum MSE\n",
    "    location <- which(result[,3] == min_cost) # find the location of the minimum MSE\n",
    "    opt_gamma <- result[location,1] # extract the optimal gamma\n",
    "    opt_sigma2 <- result[location,2] # extract the optimal sigma2\n",
    "    return(c(opt_gamma,opt_sigma2))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Development of R code for Implementation of Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theory of ridge regression can be found in section 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code for developing ridge regression function named ridge to calculate the regression coefficient\n",
    "ridge <- function(Xtrain,ytrain,lambda){\n",
    "    Xtrain <- as.matrix(Xtrain)\n",
    "    ytrain <- as.matrix(ytrain)\n",
    "    \n",
    "    # Judge if the 1 vector is already included in the Xtrain\n",
    "    if(any(!Xtrain[,1]==1)){\n",
    "        print('The 1 vector is not included but will include automatically')\n",
    "        ones <- rep(1,nrow(Xtrain))\n",
    "        Xtrain <- cbind(ones,Xtrain)\n",
    "    } \n",
    "    \n",
    "    # Calculate the regression coefficients beta by constructing matrix function Ax = b\n",
    "    I <- diag(rep(1,ncol(Xtrain)))\n",
    "    A <- t(Xtrain)%*%Xtrain + lambda*I\n",
    "    b <- t(Xtrain) %*% ytrain\n",
    "    beta <- solve(A,b)\n",
    "    return(beta)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code for developing k-cross validation used to tune regularization parameter lambda of ridge regression\n",
    "# Use k-fold cross validation\n",
    "# This procedure is the same as that introduced in the LS-SVR section but the model (here is the ridge regression)\n",
    "kcrossvalidation_ridge <- function(Xtrain,ytrain,lambda,k){\n",
    "    Xtrain <- as.matrix(Xtrain)\n",
    "    ytrain <- as.matrix(ytrain)\n",
    "    n <- nrow(Xtrain)\n",
    "    size <- floor(nrow(Xtrain)/k) \n",
    "    costs <- matrix(0,nrow=k,ncol=1)\n",
    "    for(i in 1:k){\n",
    "        if(i==1){\n",
    "            train <- (size+1):n\n",
    "            validation <- ((i-1)*size+1):(i*size)\n",
    "        } else if(i==k){\n",
    "            train <- 1:((i-1)*size)\n",
    "            validation <- ((i-1)*size+1):n\n",
    "        } else {\n",
    "            train1 <- 1:((i-1)*size)\n",
    "            train2 <- (i*size+1):n\n",
    "            train <- c(train1,train2)\n",
    "            validation <- ((i-1)*size+1):(i*size)\n",
    "        }\n",
    "        \n",
    "        Xtr <- Xtrain[train,]\n",
    "        ytr <- ytrain[train,]\n",
    "        Xvalid <- Xtrain[validation,]\n",
    "        yvalid <- ytrain[validation,]\n",
    "        beta <- ridge(Xtr,ytr,lambda)\n",
    "        ypvalid <- Xvalid %*% as.matrix(beta)\n",
    "        mse <- mean((yvalid - ypvalid)^2)\n",
    "        costs[i] <- mse    \n",
    "    }\n",
    "    cost = mean(costs)\n",
    "    return(cost)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code for developing tuneridge function using exhaustively search optimiziation algorithm called linear search\n",
    "# Note: this procedure is similiar to that introduced in the LS-SVR section but the model and parameter. Here, just\n",
    "#       one hyper-parameter, lambda. So, the linear search is used.\n",
    "tuneridge <- function(Xtrain,ytrain,lambda,k){\n",
    "    Xtrain <- as.matrix(Xtrain)\n",
    "    ytrain <- as.matrix(ytrain)\n",
    "    numl <- length(lambda)\n",
    "    result <- matrix(0,nrow=numl,ncol=2)\n",
    "    for(i in 1:numl){\n",
    "        cost <- kcrossvalidation_ridge(Xtrain,ytrain,lambda[i],k)\n",
    "        result[i,1] <- lambda[i]\n",
    "        result[i,2] <- cost\n",
    "    }\n",
    "    min_cost <- min(result[,2])\n",
    "    location <- which(result[,2] == min_cost)\n",
    "    opt_lambda <- result[location,1]\n",
    "    return(opt_lambda)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation of LS-SVR and Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the training set $\\{(x_i,y_i)\\}_{i=1}^n$, the implementation procedure for LS-SVR is first to use $tunelssvr$ function to tune the hyper-parameters such that an optimum pair of $(\\gamma_o,\\sigma_{o}^2)$ can be obtained. Second, given the optimum pair of the two hyper-parameters, the $trainlssvr$ funtion is used to obtain $\\alpha_{nx1}$ and $b$. Finally, given the $\\alpha_{nx1}$ and $b$, the $predictlssvr$ function is used to predict the testing set $\\{x_j\\}_{j=1}^m$.\n",
    "\n",
    "The implementation of ridge regression is much more straightforward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Simulated Dataset without Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4QAAAGkCAMAAABeoxsWAAAAM1BMVEUAAAAAAP9NTU1oaGh8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\n6REC5ogQMEaEgDEiBIz9B+sUBbXkcgnvAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## main program \n",
    "\n",
    "# The simulated dataset is taken from Schaal and Atkeson (1998)\n",
    "x = seq(-2.5,2.5,length.out=200)\n",
    "n = length(x)\n",
    "set.seed(1)\n",
    "y = sin(2*x) + 2*exp(-16*x^2) + rnorm(n,0,.16)\n",
    "ytrue = sin(2*x) + 2*exp(-16*x^2)\n",
    "\n",
    "# Plot the simulated dataset\n",
    "options(repr.plot.width=7.5,repr.plot.height=3.5)\n",
    "par(mar=c(4,4,1,1),mfcol=c(1,2))\n",
    "plot(x,y,ylim = c(-6,6),pch=20,col=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter space for LS-SVR and ridge regression implementations\n",
    "range = seq(-10,10,length.out=11)\n",
    "gamma = exp(range)\n",
    "sigma2 = gamma\n",
    "lambda = 2^(range)\n",
    "\n",
    "## Implementation of LS-SVR\n",
    "\n",
    "# Tuning the hyper-parameters using 10-fold cross validation\n",
    "k = 10\n",
    "result1 <- tunelssvr(x,y,gamma,sigma2,k)  # result1 include optimum gamma and sigma2\n",
    "opt_gamma = result1[1]\n",
    "opt_sigma2 = result1[2]\n",
    "\n",
    "# training the data with optimum hyper-parameters to obtain alpha and b\n",
    "result2 <- trainlssvr(x,y,opt_gamma,opt_sigma2) # result2 include alpha and b\n",
    "b = result2[1]\n",
    "alpha = result2[-1]\n",
    "\n",
    "# Predict the response \n",
    "yp <- predictlssvr(x,y,opt_gamma,opt_sigma2,alpha,b,x)\n",
    "\n",
    "## Implementation of ridge regression\n",
    "\n",
    "# here we use 10-order polynomial to approximate the fitting\n",
    "X1 = cbind(rep(1,n),x,x^2,x^3,x^4,x^5,x^6,x^7,x^8,x^9,x^10)\n",
    "opt_lambda1 <- tuneridge(X1,y,lambda,k)\n",
    "beta1 <- ridge(X1,y,opt_lambda1)\n",
    "ypr1 = X1 %*% beta1\n",
    "\n",
    "# here we use the true predictor sin2x and exp(-16x^2) to fitting, assume the true predictor is known\n",
    "X2 = cbind(rep(1,n),sin(2*x),exp(-16*x^2))\n",
    "opt_lambda2 <- tuneridge(X2,y,lambda,k)\n",
    "beta2 <- ridge(X2,y,opt_lambda2)\n",
    "ypr2 = X2 %*% beta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4QAAAGkCAMAAABeoxsWAAAAOVBMVEUAAAAAAP8A/wBNTU1o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\nEIDM/AcIQULfYDZ1pwAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"True Predictor\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot the results\n",
    "\n",
    "# set up the plot\n",
    "options(repr.plot.width=7.5,repr.plot.height=3.5)\n",
    "par(mar=c(4,4,1,1),mfcol=c(1,2))\n",
    "\n",
    "# plot LS-SVR, true function, and ridge regression using 10-order polynomial approximation\n",
    "plot(x,y,ylim = c(-6,6),pch=20,col=\"blue\",xlab = \"Predictor Variable\",ylab=\"Response Variable\",\n",
    "     main=\"10-order polynomial\")\n",
    "lines(x,yp,col=\"red\",lwd=2)\n",
    "lines(x,ytrue,col=\"black\",lwd=2)\n",
    "lines(x,ypr1,col=\"green\",lwd=2)\n",
    "legend(\"topright\",legend=c(\"LS-SVR\",\"Real Function\",\"Ridge\"),col=c(\"red\",\"black\",\"green\"),lty=1,cex=0.5,y.intersp=2)\n",
    "\n",
    "# plot LS-SVR, true function, and ridge regression using true predictor \n",
    "plot(x,y,ylim = c(-6,6),pch=20,col=\"blue\",xlab = \"Predictor Variable\",ylab=\"Response Variable\",\n",
    "     main=\"True Predictor\")\n",
    "lines(x,yp,col=\"red\",lwd=2)\n",
    "lines(x,ytrue,col=\"black\",lwd=2)\n",
    "lines(x,ypr2,col=\"green\",lwd=2)\n",
    "legend(\"topright\",legend=c(\"LS-SVR\",\"Real Function\",\"Ridge\"),col=c(\"red\",\"black\",\"green\"),lty=1,cex=0.5,y.intersp=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above (left figure) , it is obviously observed that the LS-SVR can perfectly fit the simulated dataset, while the ridge regression with 10-order polynomial performs worse in some regions. This comparison illustrates that the LS-SVR, as advanced statistical learning approach, can discover the true nonlinear relation of the data, while the ridge regression fails to reflect the true nonlinear relation of the data if the good predictors are unknown (e.g., here we use cbind(rep(1,n), $x$, $x^2$, $x^3$, $x^4$, $x^5$, $x^6$, $x^7$, $x^8$, $x^9$, $x^{10}$) as the potential predictors).\n",
    "\n",
    "From the results above (right figure), it observes that if true predictors are known (here we use cbind(rep(1,n), $sin2x$, $e^{-16x^2}$) as the true predictor), the ridge regression can give the best prediction. However, in the real world, the good predictors for physical phenomenon is typically unknown. In this case, machine learning method may be a good way to model the physical phenomenon at the expense of explaination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Simulated Dataset with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4QAAAGkCAMAAABeoxsWAAAAM1BMVEUAAAAAAP9NTU1oaGh8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\nSGFm7hEC5ogQMEaEgDEiBIz9B6sJ+u0IdRZLAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dataset contaminated by outliers, the contamination ratio is controlled by ratio\n",
    "# Note: the dataset here is the same as that in section 4.1. The only difference is in that\n",
    "#       some of data points become outliers\n",
    "\n",
    "ratio = 0.1 # contamination ratio\n",
    "numb = sample(n,n*ratio) # randomly select points from the simulated dataset as the outliers\n",
    "set.seed(2)\n",
    "y_c = ytrue[numb] + rnorm(n*ratio,0,2)\n",
    "y[numb] = y_c\n",
    "\n",
    "# Plot the dataset contaminated by outliers\n",
    "options(repr.plot.width=7.5,repr.plot.height=3.5)\n",
    "par(mar=c(4,4,1,1),mfcol=c(1,2))\n",
    "plot(x,y,ylim = c(-6,6),pch=20,col=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of LS-SVR\n",
    "\n",
    "# Tuning the hyper-parameters using 10-fold cross validation\n",
    "k = 10\n",
    "result1 <- tunelssvr(x,y,gamma,sigma2,k)  # result1 include optimum gamma and sigma2\n",
    "opt_gamma = result1[1]\n",
    "opt_sigma2 = result1[2]\n",
    "\n",
    "# training the data with optimum hyper-parameters to obtain alpha and b\n",
    "result2 <- trainlssvr(x,y,opt_gamma,opt_sigma2) # result2 include alpha and b\n",
    "b = result2[1]\n",
    "alpha = result2[-1]\n",
    "\n",
    "# Predict the response \n",
    "yp <- predictlssvr(x,y,opt_gamma,opt_sigma2,alpha,b,x)\n",
    "\n",
    "## Implementation of ridge regression\n",
    "\n",
    "# here we use 10-order polynomial to approximate the fitting\n",
    "X1 = cbind(rep(1,n),x,x^2,x^3,x^4,x^5,x^6,x^7,x^8,x^9,x^10)\n",
    "opt_lambda1 <- tuneridge(X1,y,lambda,k)\n",
    "beta1 <- ridge(X1,y,opt_lambda1)\n",
    "ypr1 = X1 %*% beta1\n",
    "\n",
    "## here we use the true predictor sin2x and exp(-16x^2) to fitting, assume the true predictor is known\n",
    "X2 = cbind(rep(1,n),sin(2*x),exp(-16*x^2))\n",
    "opt_lambda2 <- tuneridge(X2,y,lambda,k)\n",
    "beta2 <- ridge(X2,y,opt_lambda2)\n",
    "ypr2 = X2 %*% beta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4QAAAGkCAMAAABeoxsWAAAAOVBMVEUAAAAAAP8A/wBNTU1o\naGh8fHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD/AAD////zEs4UAAAACXBI\nWXMAABJ0AAASdAHeZh94AAAgAElEQVR4nO2dC7tsIBiFc4y5X/3/H3uoUITKl8pe73POnhmX\nSstKN7AaABAVFjsBAPx1YEIAIgMTAhAZmBCAyKRqwjPrUva9lqy8fp32ZmzrcW0PYTYYoqBz\nhql47luen3ZbKx8jLo5RhyLR8+HZ59qnFFn+cdkdJkwbChM2XK22Vj40XmUqOqSSDp3nIM5J\nZvjJZX+YMG2ITMheNlsrH6ZVCZBKOlQ+50GcV3MRfNfv0ibDB5Ix4c5BZ4VnPnS7PZtzYr0+\nuRBJOjqkkg6VJneqLoeujD2aj8e47vG8NFtcnv0OnxPf4HMp2ene569oT35GG/U71ffmOnv5\nzATZ/inlulL8ZPWzSdnlPbdHE96pKSzuJateQzBt+tty5SRSko74cVHyodemW2YScLzbW3wb\ndNU21s+EIbxG8epZ9xdUvnTmZNqNFM8Hxs59rjUnb3vKf9plCpXMw7PcgbV5y6+bTLGwbE+K\ni2i/UR8LO6vNzUmQTcOdMaHMk8kz5Cq2eZv3EAs+Vy1SdUu+ECYU6CYU2oxMqAk42a03odhX\n23h0JozCu2omnDmZ9iPF86EtqcZFon7mnruTWmSc+PrgFyym5G73s9Q2kgybnsxBciXPXXwv\ndY/LbCLa2MbB1PdG1C+/qp8nh/J30U0otBnJrQk43u0tlk/EL9U9dRP2S5+KCedOpv1I9XxY\nNGHbb3P/1t8bE5cqJk7yttJaPkVrYTj3vxeRpd1GSgzlS2z8NAZZ816h9jL5EUZlIviLWGdO\nxL019Zt/1ONghqI7YMblg25CoY0uty7gaLeuTdjtq208OhOG8Mp3/a16OXk4MyfTjqR6Piya\nsMnoO/9y7XXg1caz/Hz2RZxU9qxspMTwkBtfjEE2f2+iKXoVK2UIX7HOsMdL+5gkGybU0E34\n1JaZBBy27XnXmvjDxqMzYQiPC3i6fYa45k6mHUn1fDCasKtCMJnf7SVKntZaOTqc7UoNpd9p\nFEPNy0VDkDX3W7trKVbqaTLvYf5oNnpcKwYTqugm/GrLTAIO23Y81X21jUdngrH4G1YZT6Yd\nSfV8WDOhttnkcjnW0HTu6xvPBMnLzqdsBE7OENMexo/HaSEhfxXdhPoXk4DDtpzqOvHtsLGb\nCbUFEeRJ9XzosuIsmlNvXslQTNgXXmU9lq7/UpryXP2tqDgT5LPtJ6uGMlcJaWYP00fTQmGn\ny/0dTeUkWTdhacqpucJU29jzSlhONtqFVM+HLiuM44RnYwNuaAk8mNoEGIWo/OaBPLtGhDHI\nttumqw3pZ8jMHqaPk+ZimFAwY8LWES+TgNPdtN/axqMzQW5UmdqEc8rvSKrnQ5cVT9OMGXNX\npuj9an4/yv4C1PZ/PsSoj0G81t9tB9p9Pkgx5nfT0iQ+Z/YwfcgfuBJqmExY8sL2ZRRwupv2\nW9t4dCbIjaa9o9955Xck1fOhzwrj3NF+8FtvrM2NExrHyIdNy4UgP3yRqddgZg/TR8VPra6/\nHCYUmEx4WRBwupv+W9t4ZZzwLn+01au5k2k/Uj0f+qww30VRqdk2bCznSZxZ36LjXLWNhhiu\natAzQbaFwFlPU/dp3MP08erc3o35b8ycY2Ay4UcKZhJwupv+W9t4dCbIj1epbNIanl9hZ5Tf\nj1TPhyErzPcTPi/laKIn59PkbKXOHW08dH6ONhp2epyUoM1Btq2K0bJBdsMexo93k6zy8v50\n3Uvu+XFATCbkOVU9jAJOdxv91jbWz4QhvHLY5Nz5buZk2o2/ez5YZvZdnzQFADkw4TLv0ure\nUQD8CW5CdkhC55opIyPEyeM9JjFy0nlF8JhjYyMEl+tmWB4gPWtEM2GkeMOSlIIw4RIn3ktg\n2Jc+OavAhJQkpeDfNeEWkpLwoPGGJSkFYUIfkpLwoPGGJSkFYUIfHA5KbjpUfvk3OeBYO3QR\nwISUJKUgTOiDs4Rs/E3/II6TlEMKmJaCnln8vbYj2Le5jovtEaSNclA/I+NN2XjX6Ca0VvCQ\nAqaloF8WtxM666+ciLf8YKpDari5MtNLGKs6aq/gIQVMS0G/LL6w87f50z6y83NZnlFySA3d\nJaxF+2FYxtSilDhOC+wVPKSAaSnol8X85h55h893eW7lITXc3qIYJIxTHbVX8JACpqWgrwnr\n9n4s5cdoddz5QcFxkVDkwbgyI8OIaMJ6QcHDC5iWgr7V0Xf7PED+IOrvH2xSxDgo6uqorYKH\nFDAtBf0S82bl9V2fy0bD52n5MY2H1DApCb2wV/CQAqaloGdinsrjAwwTnLdHkDZJSeiHtYKH\nFDAtBb0T87jwp7+cbytv7zykhklJ6IulgocUMC0FgyfmkBo6NuuNe8/0ecx2ecfKyEMKmJaC\nMKEPjh3cbLpoNox4SiUWb1iSUhAm9EE5qLX7tvuxJCa31iUUP1m3ln/rN58EFINDCpiWgjCh\nD+6VGaZrVnf6sPG/yQKPOEk5pIBpKQgT+uBWmZFFYl8yGsrRQS6YcBeSUhAm9MG9RTESrJ6R\nEFfCnUhKQZjQB3cJ627q00jCvmQV2/D/3VeYMBxJKQgT+hDjoGBCSpJSECb0ISkJDxpvWJJS\nECb0ISkJDxpvWJJSECb0ISkJDxpvWJJSECb0waVZ348mue23JU5SDilgWgrChD549K257rcl\nTlIOKWBaCsKEPigH9c+Ivmnff63+3RDnrhxSwLQUhAl98KrMzA7iksdJyiEFTEtBmNAHt8qM\nSUL3Z7fAhJQkpeDOJiyK0PHtgnuLYmMh6hYnKaN4oSC9gvuasCiOoaGXhKMWRZ5XQigYQMFd\nTVgUB9FwQ675FaLb4tzEpBSFgtQKwoQ+bOqo9twZJqQkKQVRHfUhhiGSMCEUDBEnOmZ8MD8R\nITCxjlX/CQXpFcQQRS4kYkLgDUyYPTBh7sCE2QMT5g5MmD0wYe7AhNkDE+YOTJg9MGHuwITZ\nAxPmDkyYPTBh7sCE2QMT5g5MmD0wYe7AhNkDE+YOTJg9MGHuwITZAxPmDkyYPTBh7sCE2QMT\n5k4wE67e5gYNiQiVkWsKQkAqYMLsgQlzh9iEDvd8Q0MiaDPSXkEISAWxCV8lTLg3tBlpryAE\npIK6Ovo9s+rD9zcFYO1QYA9xRi4qCAFDQN8mfDD2qNEm3A/yjLRUEAJSEaBj5lOx8xcm3A36\njLRTEAJSEaR39MbKJ0y4FyEy0kZBCEhFmCGK92m9yQANiQiSkRYKQkAqQo0TXmDCvQiUkasK\nQkAqMG0tezBtLXdgwuyBCXMHJswemDB3YMLsgQlzx8qEz3PbSD9/9okZuGGTkSEUhIBU2Jiw\nEnOUWAkNU8QiI4MoCAGpsDDhnVXfVsI7u+wSM3BjPSPDKAgBqbAwYcnkFCbaKbvQkIj1jAyj\nIASkwsKEvCIDEybLekaGURACUmFhwpMsR9/stEvMwI31jAyjIASkwr5N+CzZfZeYgRvWbUJi\nBSEgFTa9o2d5E2e1T8zADYuMDKIgBKTCepyQnR87xQzcsB0npFYQAlKBGTPZgxkzuQMTUlAU\nESOHCQlIU0GxwuERhmQx50dRxNRwOSPDKXggAVNVECa0piiiaggTbiZVBVEdtSZVCQ8abwBS\nVRAmtCfRysxB4w1BogqqKx68g/u5U8wZkmazXiGEgkcSMFEFx7cytSLuEzNww/ZWJmoFISAV\nFia8srItQp8lu+0SM3BjPSPDKAgBqbC6lenNP9+s3CVm4IbNrUwhFISAVFjeyqR/CRszWEVr\nwdjdyqR/oQACbsBOQbU62pWjV8pkQENv9L48m+poCAUhoD+WCqp3UfAWxaskfboFNPRmNKpl\ncxdFCAUhoDe2CmLGTLI4mRAzZhIEJswfl+ooTJgi7tXRMEBDfxw7ZsIAATfg2jETCGhIBEyY\nOy4mfP2BCRdRpy/54ZCRpAqmKeChFFRWXP9OmzDuRF4/LDIyiIJJCngsBdVxwg7SCcApahj5\nlpY5lpNkM04YQsEUBTyYguq0tUddsc+nYi/CZCWpYZoSrqTJZtpaCAVTFPBgCurT1m5NGfr+\nA0/MS1XBpVTZTVujVzBJAY+loG7CZ/vY2D/QJkyxWU9jQnoF0xTwUAoOK85NZebDTvXLRsLv\nhbFKtjyWt09UwxEpSLq5OhpGwTwEzFrBYcWzlYLfFro+9fBbKjePHsCEMvciC7m1YyaMglkI\nmLeCyopb+70pHy2m4F/bSs/3XvK2R/4mLAZiJ2Uei4wMomAOAmauoF8Wl2K3T3n6HMKE7UUh\ndQ1pM9JewRwEzFxBvyzuVPtWlUnCYLPBw1CIhOYqoV9oiwpmJiBX8F++CnZ3UWgZvxpc+yY8\n+a3K/krYKvj7SRfGTsw8a3dRhFIwfQFbBf81ZKugnwmHt6J/WJW5CYUHWxf+UlaQ2IT2CiYv\nYOfBhkwV9Mzia6/bc0Xy5DXkHmyL0MRdSJyR1gomL2DNPVgU7bUwTwWVcUKnB5O8+4n6n4ut\nCX8/lxj2ovMgvyImrKHFOGEQBdV1iSrIPVi0NsxTQX3GTOCYU9SQXwDrzoUJtynsZswEjjdJ\nBVsP1sKE/7JUcFgxNNWDxfxLT0Phwbpz4S9dDV06W4LFm6SCrQfrergUZqfgsOJ7rkhvnzDF\nnJyGSkswdReumzCMgnq8CSrY98ek7kKr6ugON/UmJ6Fiu+ZDtg8jJ8qMTXV0h5t601OQNwjF\nd27C/BTc14RtNzJl4FsZd4myhF2YiAkTc6FyIeSk7ELqIQq/mOVoTugobTEMS3RdpdHSNE+s\noYKpCdNxYatgeyEclrSDhdkpuKsJW9JxoWwE6gvTvRSmYsKEXCj6YvTUJHwpdDFh4Ket/RKx\noewPHamV7mChgwkDP20tFRPKscGRWukOFtqYcK+nrSXiws6Eul4FvxRGStIiFqrs9bS1RFzY\nmVCXS1RIIyVpEQsT7va0taY6k4QNzTPVuAvjJGiZdWPt9rS1VCqk0yZhC6+QxknQMhYm3Odp\na22OSReO6xG7U3Qj9RrChOlVaNZNuM/T1qSC3deoiL7R8dL2UphA4iZYTlsL/rQ10W/V5tG/\naRG2P5MmoViYZP+a3bS14E9bGxRM4olnpqK865uJnrgRliYM/bS17q7LRkNDbX53Jn2jcin7\nOd0dus9h2Jkw9NPWNAXjn+jjvtFuaV4KKndRuDyryzPmQcIfn277L2atoa2NGldML4WThmOh\n/djlGCzuogii4JwJf93XmAqauxYMg4VJKziscHlWl3fMQrei6FwYsUpjGCTsEC5UN9Uf5aUl\nergghD2SdWOFUdBQHe0VFB6Mp6D5Qti7UN00ZQWVFQ7P6vKMuSj6x2L9pA07TUnjtME0SNit\n4rfaa5sWMunDT03PopvGHzDBFle3IAqOqjIjBYsipoJznXutOzNSUKwgHZSYjblQ4Rr+627H\npDx4u7DmTcjXTCWcnnrK6lotT8OwbMJwCuql6ExWRFJwxoR8TUYKihWsvH6Cx9wdOR/Jkrkx\nmJDs4O3CEgmZW9Wm8Nf9kkHy9IqH0XRHMNx+Ucc2YTgFDaWoqmAAF9orODPaXMhnXXS/UldQ\nrDix4ZnowWLu1NMoBhfSHL5dUMWCCWVCf0o3vCxBp8lnSqkdVsEVE4ZTUDfhvzGsUCCJ0EHB\nuSkfIqEGBSfJ/5eCgnLF59o+Fv36Dhpz50HNjUqNlCQH7CWc7Zfp1g7d8IXmv5EhFQ0J0j/L\nSpswmIJadXR6Emv67azgXL+M6sJeQXPS9Vkj0TtmXk2Lnp3u1E9I0KujWhtZmFErTQkitK7M\nLHTkD8XEb2Q/mcyuIkY/VXOW9UjCKKhdCcWgkqLgxIcEMVoruDD3sUsaV9BQXnQKdssJkr2K\n3V0Uj7Z/+0Jbqekj0LQb2he/1obK6b0d22b9kneKSdVZLTraDfr20OhyGAwrpwdQcNyonyo4\nOr23Y6vgkneKSdV5VsHR5TAYtrcyfW9N46IMErOq3fC7y4S+nbxbX/d8bVRJnHr56zvQxOev\nF7He5XJoGT65ghMTGhRUfbifgvO10T5x4+vfnIK7XA4d7id8hpoxM2inLxFdWYz0amiTsMXD\nLKbI5cNaqaG4quqdNCHSa70lrYKj3lHxqS9ps4L6amjBsmscFRQujNox0xHwSmiqZfRLmkJN\ntMB203DxQliPNZxZ+Rs64ORFM1h6E7gSLipY/7TL4Q4sXghrDwVD10oTaBOakbkja6Ry6jRp\nAmawuFoMNRfzquAu1Ir6+G1CM8ohK22vHbCoPjoqGLhSajN3NHTvqIleMH70Wh9HYCU3Vtn6\n4lUdsJ4MHW7E0YRhFFwtRdXDlUNO3SralOi0Q3xbMCsYsla6ZsJXO8pUBh4nnKLWFPjBs/5y\nErg8XauNWiDTN3UhWcpdTBhMwfVSVD3cQg781sEVXKuNWmBUUF7MN6dvSgozZgxoEv4Tg1Di\nYmisyFMmi7DvQhGRh9wXI/XGq4GDCXeZMWNgqpOo1QVXcOuFUENXUClGasrr+erc0VuQF1Es\nxczRdJJlkNJRKrfxiXd5L4ILoULfycYjllVqnv5t56GDCcMp6FQdrfsButAKElwIFXQFZZWa\nQEGNFROGeAnFWswC7Qg78frBcbGFTy6s7EVrwrpr3tfdlDemTqn0DtbBhOEUdOiY6RYMNuwX\nuMe7shetCccKapOAqFzoME5IjFsEmgtrZZqbI2t7kQ+ti+n5XcS6C70Dde8dDYBjvEWhnMUB\nFSTvxdQV1F1IE0MmJhzutu9HKzIxoRSxl032z+xWHQ2HW7zigLWLYSYmHCvYFyN7VUcD4hKB\nPPapC92jXd5rcd6oP/J2td6F9bZmvX6eZWHCzjoTF7rHu+bB4ArKMcPdOmYC4mrCQmkYdlOK\nfOJd2GvpLqZt/BRGs0k9jkJv9uRkQlkb6LoZAyhI3CTs0RTUh+59juJnpWBSJhSln949Q5+i\nwvzQXyf+rSLugfrVSsPIPZLsTFgrHuxcSJ8k+TJQ+oBl6LoLNyiYpQnrYTJtrbqQ0ouF8en3\na6y7zsAwG9arXZSjCdWumG7ou1tMRTF+KeH2ACc/+RH8Rvecu0fpbsLnua1Bna2eVfK6nfns\nkPN1pXPc49zp7/Aq5B1ChI1juwvhur+08NTA5OmnXBJ9JdSfVWSVkSEU9DG/PNjuYkiuIO2F\n0KygVqXezYSVOOVZua7h96Tc7rr8zHU/DWvVhX7HPx+6/iw1y6vafFq1xCmlZmdEdTKGC+4m\nDKKg1xW4H/culIuJT0Dm0EdZszG0eQULLf07VEfvrPq2Et4tHh17ZeVDzFL8PMvlx1z616IG\nG9K50NJxDt3fRgkVJf+NzkH74xj1PaxnZBgFN0+TZn0WbApJhbRbZk3BcSnicBw/OwWHFSX7\niuEzi+77kg0Thd/Ld69t0FD4ob/ByVdDOsMZMVVm+mmHxfjhCbPHMVk8Lu1tVAmh4Ma2qHYx\n2RbUAG3f6JqC2q0hDgp6mJBXZCwl1DaZbq89nGU1sAX+dT50vc1w0XHEg4SGZn33TSlKu/6J\nmQOZLnY3IZ2CZALqpzGVCakHCS0VVBYZA5kudjbhSZajb3ZaTfU+V8J+MrC8lqyJaHOpKwrq\neaPzjNoVypPKZjbVFo1L+/WMDKMgxV17XTcphQ15h0EcBYdFc5vqi36WCk7ahE0L4b6auKZF\n8RSN/4BtwlqdxzdXeXSsY4oM3es9rkKWQjwJanDhzJb6islwtHWbkFhBEhPKZ1sSVEkL0Te6\nMRSH2HoF5Wllr6CHCeuzVW+noFKqK6fFe2i2azj0sa1hFdyer8PuVeEN26WLwVTBcfXNIiOD\nKEhQHRUKkriwkE/13ZYol/jkF5F+bdEkZRMPWio4Hidk54dV4l5Xrnh5vtGPE6oU3d0wsn/G\n3Xh6aPuaUIl3uBjObTL66WHCIApubX4pChK4UF4IIym4eLpNFPQyYRBoIpBn8fiwPMKJ4UFp\nfv7mDdtiY/Iy9kxmzMxB1T0zfuHSTkx7Sdd3+dkqmIkJ+9P4t2nyRaQLYd9HqF0Mlw9j0vmQ\nuQnrvpdxo4JRLoSdgroLVw5jnEwbE95PTSv9xE60N2mTaqhMxvQPI4KCIupfdzEsuiULO0yu\nmBYZGURBMvMrF5MNCv7bsVtmFLU6mbRbsrDDpKiwMCF/bnP7Zh+Wpoa83Jm40EXLWC3CPnp+\nZ4V04cqJOO2FX8/IMAoSXoG5gmMXuin4L9KFUEYv76woinUFvUxYsQcfYXpYda5ZQ1qLki8O\nHA7foUTtaoSxFOQpGFy4asLxkvWMDKMg9bj4aDKpo4J85gZpilwYLobrCk6LCssZM+92xCjU\nuyi209mon8WmutFiX7EzYYKc4PWZX989syihYTjabhYMvYLEpegw3OSlYMwLYa+g7JdfVPDn\nbcIze6Zuwm6sQpdwtTwt5L6GAmonRArEvb6rGhr6UO1MSK8guQnlkKFoGrsp2FZGM1HQkEqr\n6uj72c5fSrs6WujdM5Mr4uKeCZiw/g3dM/OJ/nmZMIyC1NXRTsGhbWit4L8UTKgouHCHlimR\ndh0zjN3aYnTX94m4UQwiLpnQkDOxPdiX9b++KBWJNqnoZ8IwChIPjQwK/lNPYysF2wZhUgr+\nm1fQz4T1XcwhPNlNuLCFenirKGrlnQ+dcJqC/HlZhh1ZXAk7sTQXGqthJg9aDVGEUJB8fHJQ\n8F9fK7VR8B+/rS0hBfu27drtE5LsB+s1iu7luEM51OfD8LAswy5RFexpzzCtWVGM15vmmOY+\nWK/RKTj00KwqyOuiaSk460L5FOHRXscyYUv3ABr1LO7Ek8/L0raW10ESCbfeCiDSODNu/ZMe\nHEdyKBO2SAUVHy4o+K+/Dqak4JD+8UpTT9PhTNi3LJSGYV98yvaz1EvU+dpNiYrRtS6Edbon\nk07b9j/uQVOnzdFMqLcNxV1CMwqKDZJUUEv/sMbY02Rjwlv/7J9t6ZuNgJKhfd92lY7rL3xC\ngyithi2LyQ1eW6LeGMjwYEs1/eIg+lqq610UQRQMakKpoDyPjQrKdakqOEq/ULA/7Vzvorgx\nlpEJu4PUXDgcs1gplg3dOHTF6GYJa6niPzEPa6DgNzEZNFzPyDAKBqyOqgr+G+jX9ktSVnCS\nfhl6UYzvmbEwoc392B6E1rB/u2936P2azoWKgjSNehoFZVhdRWvo5h16TbV41jMyjILhqsFj\nBf+ZSV3BYjH9NgrqM2YCEFrDWmqovRFwQFlDpeD2Zr0aUtHZUL4/Rg7jT4trx4c3kRGwLWpW\nUD2D81CwT38tS1U1/cOmFiY8syCveg2soZ4LqheHJZMmYzIMZ12X1n+dB+txtXI9I8MoGLJD\n6CgKDuVIfwC1g4LDik9ZhXjda1gNlbonMzO83iNVBQu99Pynpl7ZeD0jwygYtFf2MArOpl/Z\n2Ko6atpxM6F71rVs0HKiW0JZ+aBlSLpSg5lTwaY6GkLB4EMjx1BQwVnB7E2oNLJ1xQYF08Wg\nYDGX7sOaEArmOliv0B+tfugZKKhWx6ZKjjY92mC9wl9X8AAmHDBpuGP0vshk/l0TDvxNBdUV\nj/aBsJZPrbQmsoZ7xu6NWhubvQDYZGQIBfc1/59UUFnRPZOZ9J7eyBruGvl2TDdmdVhkZBAF\nd74C/0UFhxV3Vrb3glq9ycCBmCbMFtMhrGdkGAWjmjBbnBQcVpzke3ps3unjQAQN941xFH2w\noNczMoyCe7dF/6CChmlrmQ1RjIisYLjoHaatZTZEMeLvKWi6Ei6+b5As5gMStCrlciWkVPAv\nCRhJwYO1CeMS2YSHaBPGJbYJD9E7Gpm41dFD9I5GJnJ1tK4fDm+32x7zIYnaMRNGwb8lYOSO\nmb1jBm78iRkzhwYmzB6YMHfsq6MXq6c3fy9Ny0NuudwhDg2JsK6OEisIAalw6Zg5rwf3LZUt\nYcJdcOiYIVUQAlJhYcKrQwf3td3mey95NxxMuAvrGRlGQQhIhYUJS4dJT6XY7VOePjDhTqxn\nZBgFISAVxNPWuk2+VQUT7gTttDV7BSEgFVbV0a4cXW9SnPrnep0qmHAfbKqjIRSEgFTYdMzc\neIviVVrMt7izi/z2YRVMuAsWGRlEQQhIhVV1dO5RbQau/QbPlW2hIRE2VcwQCkJAKqhNWL/7\nGs/nMtnWJSBgCbEJFxWEgCHAjJnswYyZ3IEJswcmzB0bE95PTc3kxE4uj1Jfr6xAQyIsMjKI\nghCQCgsTPls5+GQmBw1hwt1Yz8gwCkJAKixMWLEHn2vxcLknFCbcjfWMDKMgBKTCcsbMm12t\n5ltoO3nGDNywmwVDryAEpMLShGf2hAkTxc6E9ApCQCqsqqPvZ/uYLlRH08SmOhpCQQhIhV3H\nDGO3VhWrm0I3xwzcsOqYCaAgBKTCaoiibNsT9QnPCUoSmyGKEApCQCowWJ89GKzPHZgwe2DC\n3LEy4fPM+9c++8QM3LDJyBAKQkAqbExYiSnzrISGKWKRkUEUhIBUWJjwzqpvK+Fwu2fYmIEb\n6xkZRkEISIWFCUv2FYNGeb8a7bCsZ2QYBSEgFZYzZmDCdLGbMQMTpouFCU+yHM37Tb354Prm\nkfWMDKMgBJyDTMFJmxDvJ9wH53dwWbcJ8X7CfaBTUFlxxvsJ98P9bZQWGRlEQQhohlDB8Tgh\n3k+4D0FMGERBCGgmkAmDAA3NBKiOhgECzhCkOhoGaDgDfcdMGCDgHAE6ZnreFm/WIogZuOGQ\nkaQKQkAq1kz4qprmPH+TwfuMccIkWcnIYApCQCpWTPgSvWrv+tO27K+7xAzcWM7IcApCQCpW\nTFi1sl1Z1d6bff7ObUsbM3BjOSPDKQgBqVgxoai/MFay83uvmIEbyxkZTkEISIWlCZ2e3bwx\nZuCGnQnpFYSAVFiacM+YgRt2Jtw7XmAPTJg9MGHuwITZAxPmzqoJg70WEhoSsWbCUApCQCpg\nwuyBCXMHc0ezB3NHcwcmzB6YMHdgwuyBCXMnmAnxUp+9CJWRawpCQCpgwuyBCXOH2IQOfXHQ\nkAjajLRXEGaMVYEAAAhUSURBVAJSQWzCVwkT7g1tRtorCAGpoK6Ofs+s4i88QHV0L4gz0lpB\nCEgFfZvwwVj7VC+YcC/IM9JSQQhIRYCOmU/V3j0KE+4FfUbaKQgBqQjSO3pj5RMm3IsQGWmj\nIASkIswQxftkbtMHm4j6lwmSkTMKQsAQhBonvOBKuBeBMnJVQQhIBaatZQ+mreVOKBOuV1ag\nIRGBMhJVmd2ACbMHJswdmDB7YMLcgQmzBybMHZgwe2DC3IEJswcmzB0MUWQPhihyBybMHpgw\nd2DC7IEJcwcmHHB9y3EiwIQ9R1Pw75mwKPLUECbsOJyCf86ERZGphjCh5HgKwoS5ABNKjqfg\nnzPh8SozB413nsMp+PdMeLhm/UHjXeBoCv5BE2YKTJg7MGH2wIS5AxNmD0yYOzBh9sCEuQMT\nZg9MmDswYfbAhLkDE2YPTJg7MGH2wIS5AxNmD0yYOzBh9sCEuQMTZg9MmDswYfbAhLkDE2YP\nTJg7MGH2wIS5AxNmD0yYOzBh9sCEuQMTZg9MmDswYfbAhLkDE2YPTJg7MGH2wIS5Q23C77Vs\n/t5OjFUPz5iBG8QZaa0gBKSC2ISfkrH62/xpqfxiBm7QZqS9ghCQCmITXtj52/y5fBo1L+zq\nFTNwgzYj7RWEgFQQm5Cxr/zT1GtY6RUzcIM2I+0VhIBUkJuw+VMy5Yd7zMANahPWlgpCQCrI\nq6PvplHf/mnLUTQp9oC6OmqrIASkgtiEb1Ze3/W5bDR8ntjTK2bgBm1G2isIAamgHqJ4yn61\nlptfzMAN4oy0VhACUkE/WP+4nFr9zrePIVAV3wiADnlGLigIAUOAGTPZgxkzuQMTZg9MmDuh\nTLheWYGGRATKyFUFISAVMGH2wIS5AxNmD0yYOzBh9sCEuQMTZg9MmDswYfbAhLmDIYrswRBF\n7sCE2QMT5k5EEwIiQisFAUMzm8V76rnMpqTE2zlm1IkBBaMFQQUkzB0oGC0IKiBh7kDBaEFQ\nAQlzBwpGC4IKSJg7UDBaEFRAwtyBgtGCoAIS5g4UjBYEFZAwd6BgtCCogIS5AwWjBUEFJMwd\nKBgtCCogYe5AwWhBUAEJcwcKRgsCALAFmBCAyMCEAEQGJgQgMjAhAJGBCQGIDEwIQGRgQgAi\nAxMCEBmYEIDIwIQARAYmBCAyMCEAkYEJAYgMTAhAZGBCACIDEwIQmZRMeD+x8vr1393vWK7l\nplh9o+W7bjvg9ICCXiRkwit/fVTpe0hvv5eHVTzWk2ek3tG2bDzg9ICCfqRjwje7fNtS6eK5\ne+mVly9Wvtt9X36x+kbLd912wOkBBT1Jx4RnkRTPHLmzymvPK3s2fx/s5hWrd7Qt2w44QaCg\nJ8mdAZ5HxK5+e57Zp26LtLNXrN7RqkEkJ8FGoKBzABv3p+bLKq/93p55wbYVZr7RDvgecLJA\nQWdSM+GdVy68iCHhtl1bNhxwmkBB9wRs3J+YT+lZq6gzlXDLAScJFPRIwNYASPmWG67sOUq4\n6YBTBAr6JGBzCJtTwBHfK9fRHnVnr7ws40rofMApAgU3kpIJP6fq472zZ16KvrWPb9+ab7QC\njwNOESi4kfgm7Hlu7GXyyssbb1U/2XXfaDlbDzg5oKBnAigCIeGz9YCizLfYIOHmA04NKOib\nAJJQKLgwplZN3PHb88Tj3Ls3oWXzAacGFPRNwMb96WBxJPzyOfi+cXpHWxMccGpAQd9wNu4P\nANgITAhAZGBCACIDEwIQGZgQgMjAhABEBiYEIDIwIQCRgQkBiAxMCEBkYEIAIgMTAhAZmBCA\nyMCEAEQGJgQgMjAhAJGBCQGIDEwIQGRgQgAiAxMCEBmYEIDIwIQARAYmBCAyMCEAkYEJAYgM\nTAhAZGBCACIDEwIQGZgQgMjAhABEBiYEIDIwIQCRgQkBiEyuJhRvSC0vn+WNJq9hfc5v/WKn\n/vuJGTYcBaX8PM7bdvcDCg6RR4x7C92LisslDacSnpaOt2Qv+e3DypngzD9hQneg4BB5xLi3\nIDLtW7Gll5VPc3Yxr299YNfFYKdhwYTuQMEh8ohxb0Fm2tdY4I02WlyiMARWssVK0iQsmNAd\nKDhEHjHuLXSZJuor3xM7Nz/uJ1bexfJr2RSFQ2Wm+Vl9ZBWoFlue7mJ/uW9LJdsRL1Y1f5/n\npq50VTbiu6pLm/K2/6pHD1aBgj2Zm5AXfYw12dpk5Zkr1OZ+IwbjS+WG/Gf57SWs+i27fTlP\nduGfl1bKm2iyXIeN2l3HS7tQai16sA4U7MnbhB/eomjy7Vu3AjQfX14WPlj5rt9lJ+GjXXMR\nm7a7dasf/b6Skg2Bs3b1Q37lG4nglKVKKFr0wAIo2JOvCWXf2rf9zvvEzqzN529bNTnzJc9O\nQv5TFrliyydfXfX7Sq6tHo1CVyWeut9oaDfIpSKUcxdJHz2wAAoOSdw7QiLUUSbGlEVM6dXu\nvo7yfrx64M3rIhV781+f562SYik7m5aK/330wAIo2JPrKaPmFJ2E9akpDL9yyLfqFVElNC6F\nCd2Bgj25njJGCU1L3CS8s1vTcucdZBd2uj8/Y7HMSw0hgRWgYE+uZ45BwvPQohZfX13OVjMt\nivMk49sy9MSbBmKNUax+6avWWhToknEBCvYcyIS8x6wpCc9tvmp9a/e22+sq+tY+9ahvTQ/3\nwrpe7laht6HtoCwVoTzFGiV6YAEU7DmQCWVln89F5CM+l77g60aZmjKSz6lQR5n0cJ+MyfLw\nKhsIL01Cbeml/Xau1UiWp0KCASjYcyQTthMemJyVfxvNt2iyul3xOomJTfeyn28xCrjsZz41\nAlUvtcbDP7SlV1behlCU6MEqULAnVxMCcBhgQgAiAxMCEBmYEIDI/Af7jw+U4fvoMwAAAABJ\nRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"True Predictor\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot the results\n",
    "\n",
    "# set up the plot\n",
    "options(repr.plot.width=7.5,repr.plot.height=3.5)\n",
    "par(mar=c(4,4,1,1),mfcol=c(1,2))\n",
    "\n",
    "# plot LS-SVR, true function, and ridge regression using 10-order polynomial approximation\n",
    "plot(x,y,ylim = c(-6,6),pch=20,col=\"blue\",xlab = \"Predictor Variable\",ylab=\"Response Variable\",\n",
    "     main=\"10-order polynomial\")\n",
    "lines(x,yp,col=\"red\",lwd=2)\n",
    "lines(x,ytrue,col=\"black\",lwd=2)\n",
    "lines(x,ypr1,col=\"green\",lwd=2)\n",
    "legend(\"topright\",legend=c(\"LS-SVR\",\"Real Function\",\"Ridge\"),col=c(\"red\",\"black\",\"green\"),lty=1,cex=0.5,y.intersp=2)\n",
    "\n",
    "# plot LS-SVR, true function, and ridge regression using true predictor \n",
    "plot(x,y,ylim = c(-6,6),pch=20,col=\"blue\",xlab = \"Predictor Variable\",ylab=\"Response Variable\",\n",
    "     main=\"True Predictor\")\n",
    "lines(x,yp,col=\"red\",lwd=2)\n",
    "lines(x,ytrue,col=\"black\",lwd=2)\n",
    "lines(x,ypr2,col=\"green\",lwd=2)\n",
    "legend(\"topright\",legend=c(\"LS-SVR\",\"Real Function\",\"Ridge\"),col=c(\"red\",\"black\",\"green\"),lty=1,cex=0.5,y.intersp=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above (left figure), it is obviously observed that both the LS-SVR and ridge regression are nagatively affected by the outliers, leading to worse prediction compared to that without outliers. However, it seems that the levels of negative interferences by outliers for LS-SVR is less than that for ridge regression. In this way, the LS-SVR performs more robust than ridge regression.\n",
    "\n",
    "One of the interesting results is that the right figure illustrates that if the good predictors are used for ridge regression, the negative affect by outliers for ridge regression is less than that for LS-SVR. In this situation, the ridge regression performs more robust than LS-SVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Theory of Weighted LS-SVR and Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the numerical results shown above, it observes that both LS-SVR and ridge regression (the good predictors are unknown) are sensitive to the outliers. In order to overcome this problem, the weighted versions of LS-SVR and ridge regression are developed. The detailed theory about them are presented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Theory of Weighted LS-SVR\n",
    "\n",
    "The detailed information about the formulation of weighted LS-SVR can be found in Suykens et al. (2002). Here, we briefly summarize the formulation and present the mathmatical equations necessary for the code developement, which can be found as follows.\n",
    "\n",
    "Given the training set $\\{(x_i,y_i)\\}_{i=1}^n$, where $x_i \\in R^{p}$ and $y_i \\in R$, the objective of the weighted LS-SVR is to find $w \\in R^{h}$ and $b$ to minimize the following objective function:\n",
    "\n",
    "\\begin{align*}\n",
    "min J(w,e) = \\frac{1}{2}w^Tw + \\frac{1}{2}\\gamma\\sum_{i=1}^n \\omega_i e_{i}^2                        \n",
    "\\end{align*}           \n",
    "\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$y_i = w^T\\varphi(x_i) + b + e_i, i = 1,...,n$$\n",
    "\n",
    "Where $\\omega_i$ is a weight, and other variables keep the same as those introduced in section 2.1 \n",
    "\n",
    "The Lagrangian function for the equations above can be established as follows:\n",
    "\n",
    "$$L(w,b,e;\\alpha) = J(w,e) - \\sum_{i=1}^n\\alpha_i\\{w^T\\varphi(x_i)+b+e_i-y_i\\}$$\n",
    "\n",
    "The Karush-Kuhn-Tucker(KKT) conditions for optimality are given by:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\frac{\\partial L}{\\partial w}=0 \\rightarrow w = \\sum_{i=1}^n\\alpha_i\\varphi(x_i)\\\\\n",
    "&\\frac{\\partial L}{\\partial b}=0 \\rightarrow \\sum_{i=1}^n\\alpha_i=0\\\\\n",
    "&\\frac{\\partial L}{\\partial e_i}=0 \\rightarrow \\alpha_i = \\gamma \\omega_i e_i, i = 1,...,n\\\\\n",
    "&\\frac{\\partial L}{\\partial \\alpha_i}=0 \\rightarrow w^T\\varphi(x_i)+b+e_i-y_i=0, i = 1,...,n\n",
    "\\end{align*}\n",
    "\n",
    "After elimination of $w,e$, one can obtain the following matrix equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "0 & 1_{1xn}\\\\\n",
    "1_{nx1} & K_{nxn}+\\frac{1}{\\gamma\\omega}I_{nxn}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "b\\\\\n",
    "\\alpha_{nx1}\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "0\\\\\n",
    "y_{nx1}\n",
    "\\end{array}\n",
    "\\right ]\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\omega \\in R^n$ is a weight vector, other variables keep the same as those introduced in section 2.1.\n",
    "\n",
    "The weight function used in this project is a function of residuals estimated by LS-SVR, which is given by:\n",
    "\n",
    "1. if $|e_i/\\hat{s}| \\leq c_1$, $\\omega_i = 1$\n",
    "\n",
    "2. if $c_1\\leq |e_i/\\hat{s}| \\leq c_2$, $\\omega_i = \\frac{c_2-|e_i/\\hat{s}|}{c_2-c_1}$\n",
    "\n",
    "3. otherwise, $\\omega_i = 10^{-4}$\n",
    "\n",
    "where $c_1 = 2.5$, $c_2 = 3$, $\\hat{s}$ is robust parameter taken 1.483 times median absolute deviation of residuals, denoted as $\\hat{s} = 1.483MAD(e)$.\n",
    "\n",
    "The implementation procedure of weighted LS-SVR is summarized as follows:\n",
    "\n",
    "1. Compute the residuals $e_i = \\frac{\\alpha_i}{\\gamma}$.\n",
    "\n",
    "2. Compute the $\\hat{s}$.\n",
    "\n",
    "3. Determine the weight $\\omega_i$ based on $e_i$ and $\\hat{s}$.\n",
    "\n",
    "4. Solve the linear system of weighted LS-SVR to compute the new $\\alpha$ and $b$.\n",
    "\n",
    "5. Use the new $\\alpha$ and $b$ to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Theory of Weighted Ridge Regression\n",
    "\n",
    "\n",
    "The weighted ridge regression is similar to weighted ordinary least squares method by adding a regularization parameter $\\lambda$ to penalize the regression coefficients for avoiding the overfitting. The detailed formulation of weighted ridge regression can be easily found online. Thus, here we briefly summarize the mathmatical equations necessary for code development.\n",
    "\n",
    "Given the training set $\\{(x_i,y_i)\\}_{i=1}^n$, where $x_i \\in R^{p}$ and $y_i \\in R$, the objective of ridge regression is to find regression coefficients $\\beta \\in R^{p+1}$ that minimize the following objective function\n",
    "\n",
    "\\begin{align*}\n",
    "min J(\\beta) = \\sum_{i=1}^n \\omega_ie_{i}^2 + \\lambda \\sum_{j=0}^p\\beta_{j}^2                    \n",
    "\\end{align*}  \n",
    "\n",
    "subject to:\n",
    "\n",
    "$$y_i = \\sum_{j=0}^p\\beta_jx_{ij} + e_i, i = 1,...,n$$\n",
    "\n",
    "where $x_{i0} = 1$, $\\lambda$ is a regularization parameter.\n",
    "\n",
    "The regression coefficient $\\beta$ can be obtained using the following equation\n",
    "\n",
    "$$\\beta = (X^T WX + \\lambda I)^{-1}X^TWy$$\n",
    "\n",
    "where $W$ is a diagnal matrix consisting of $\\{\\omega_i\\}_{i=1}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Development of R Code for Implementation of Weighted LS-SVR and Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 R Code for Weighted LS-SVR\n",
    "\n",
    "From the theory of weighted LS-SVR introduced in section 5.1, the only difference from LS-SVR is about the weight $\\omega$, robust parameter $\\hat{s}$, and new $\\alpha$ and $b$. Two extra new function called $weightfun$ and $trainwlssvr$ are developed for computing the weight $\\omega$, new $\\alpha$ and $b$. The function $trainwlssvr$ is similar to $trainlssvr$ except there is extra variable, weight, is added and the expression of Gamma is modified. The detailed information is commented in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code for developing the weight function\n",
    "weightfun <- function(residuals,s_hat){\n",
    "    # Calculate the weights\n",
    "    n <- length(residuals)\n",
    "    weight = matrix(0,nrow=n,ncol=1)\n",
    "    for(iw in 1:n){\n",
    "        if(abs(residuals[iw]/s_hat)<=2.5){\n",
    "            weight[iw] = 1\n",
    "        } else if(abs(residuals[iw]/s_hat>=2.5 && abs(residuals[iw]/s_hat)<=3)){\n",
    "            weight[iw] = (3 - abs(residuals[iw]/s_hat)) / (3 - 2.5)\n",
    "        } else {\n",
    "            weight[iw] = 0.0001\n",
    "        }\n",
    "    }\n",
    "    return(weight)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code for developing the weighted LS-SVR\n",
    "trainwlssvr <- function(Xtrain,ytrain,gamma,sigma2,weight){\n",
    "    Xtrain <- as.matrix(Xtrain)\n",
    "    ytrain <- as.matrix(ytrain)\n",
    "    weight <- as.vector(weight) # weight variable \n",
    "    n <- nrow(Xtrain)\n",
    "    Ktr <- kernelmatrix(Xtrain,sigma2)\n",
    "    ones_col <- matrix(rep(1,n),nrow=n,1)\n",
    "    ones_row <- matrix(rep(1,n),nrow=1,ncol=n)\n",
    "    Gamma <- (1/gamma)*diag(1/weight) # Gamma is changed to incorporate the weight\n",
    "    H <- Ktr + Gamma\n",
    "    eta <- solve(H,ones_col)\n",
    "    nu <- solve(H,ytrain)\n",
    "    s <- as.numeric(ones_row %*% eta)\n",
    "    b <- as.numeric(t(eta) %*% ytrain / s)\n",
    "    alpha <- nu - b*eta\n",
    "    result <- c(b,alpha)\n",
    "    return(result)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 R Code for Weighted Ridge Regression\n",
    "\n",
    "The weight function used is the same as those in weighted LS-SVR. An extra function named $wridge$ is developed to calculate the new regression coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code for developing weighted ridge regression function named wridge to calculate the new regression coefficient\n",
    "wridge <- function(Xtrain,ytrain,lambda,weight){\n",
    "    Xtrain <- as.matrix(Xtrain)\n",
    "    ytrain <- as.matrix(ytrain)\n",
    "    weight <- as.vector(weight)\n",
    "    \n",
    "    # Judge if the 1 vector is already included in the Xtrain\n",
    "    if(any(!Xtrain[,1]==1)){\n",
    "        print('The 1 vector is not included but will include automatically')\n",
    "        ones <- rep(1,nrow(Xtrain))\n",
    "        Xtrain <- cbind(ones,Xtrain)\n",
    "    } \n",
    "    \n",
    "    # Calculate the regression coefficients beta by constructing matrix function Ax = b\n",
    "    I <- diag(rep(1,ncol(Xtrain)))\n",
    "    W <- diag(weight)\n",
    "    A <- t(Xtrain) %*% W %*% Xtrain + lambda*I\n",
    "    b <- t(Xtrain) %*% W %*% ytrain\n",
    "    beta <- solve(A,b)\n",
    "    return(beta)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of weighted LS-SVR\n",
    "\n",
    "# Calculate the residuals and robust parameter s_hat\n",
    "residuals = alpha / opt_gamma\n",
    "s_hat = 1.483*mad(residuals)\n",
    "\n",
    "# Calculate the weights\n",
    "w <- weightfun(residuals,s_hat)\n",
    "\n",
    "# Calculate the new alpha and b\n",
    "result3 <- trainwlssvr(x,y,opt_gamma,opt_sigma2,w)\n",
    "b = result3[1]\n",
    "alpha = result3[-1]\n",
    "\n",
    "# Predict the response \n",
    "ypr <- predictlssvr(x,y,opt_gamma,opt_sigma2,alpha,b,x)\n",
    "\n",
    "## Implementation of weighted ridge regression\n",
    "\n",
    "# Calculate the residuals and robust parameter for ridge regression\n",
    "residuals_r = y - ypr1\n",
    "s_hat_r = 1.483*mad(residuals_r)\n",
    "\n",
    "# Calculate the weight for ridge regression\n",
    "w_r <- weightfun(residuals_r,s_hat_r)\n",
    "\n",
    "# Calculate the regression coefficient based on the weighted ridge regression\n",
    "beta_r <- wridge(X1,y,opt_lambda1,w_r)\n",
    "ypr1_r =  X1 %*% beta_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4QAAAGkCAMAAABeoxsWAAAAOVBMVEUAAAAAAP8A/wBNTU1o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\nAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "options(repr.plot.width=7.5,repr.plot.height=3.5)\n",
    "par(mar=c(4,4,1,1),mfcol=c(1,2))\n",
    "plot(x,y,ylim = c(-6,6),pch=20,col=\"blue\",xlab=\"Predictor Variable\",ylab=\"Response Variable\")\n",
    "lines(x,ypr,col=\"green\",lwd=2)\n",
    "lines(x,yp,col=\"red\",lwd=2)\n",
    "lines(x,ytrue,col=\"black\",lwd=2)\n",
    "legend(\"topright\",legend=c(\"WLS-SVR\",\"LS-SVR\",\"Real Function\"),col=c(\"green\",\"red\",\"black\"),\n",
    "       lty=1,cex=0.5,y.intersp=2)\n",
    "\n",
    "plot(x,y,ylim = c(-6,6),pch=20,col=\"blue\",xlab=\"Predictor Variable\",ylab=\"Response Variable\")\n",
    "lines(x,ypr1_r,col=\"green\",lwd=2)\n",
    "lines(x,ypr1,col=\"red\",lwd=2)\n",
    "lines(x,ytrue,col=\"black\",lwd=2)\n",
    "legend(\"topright\",legend=c(\"Weighted Ridge regression\",\"Ridge Regression\",\"Real Function\"),\n",
    "       col=c(\"green\",\"red\",\"black\"),lty=1,cex=0.5,y.intersp=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Discussion\n",
    "\n",
    "From the results above (left figure), it is obviously observed that the weighted LS-SVR (WLS-SVR) (green line) improves the fitting of LS-SVR (red line) when the dataset is contaminated by the outliers. Specifically, the improvement done by WLS-SVR is achieved across all the regions contaminated by outliers. \n",
    "\n",
    "For the weighted ridge regression (right figure), the improvement done by weighted ridge regression is not obvious compared to the result obtained by ridge regression. Specifically, the relatively obvious improvement is located at one end, while other regions contaminated by outliers are not improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "\n",
    "From this comparision studies, the following conclusions are made to answer the questions we have in the motivation section.\n",
    "\n",
    "1. When the true predictors are unknown, the LS-SVR is an effective tool capable of modeling the nonlinear behavior of physical phenomenon at the expense of explaination. On the contrary, the ridge regression can explain the fitted model but typically gives poor prediction. \n",
    "\n",
    "2. When the true predicthors are known, the ridge regression performs even better than LS-SVR (note: the true predictors are not used for machine learning). However, the true predictors for some physical phenomenon are always unknown. Almost all of the disciplinaries aims to explore what are the true predictors affecting some physical phenomenon. In this situation, machine learning methods might be a potential way to assist people to understand how some physical phenonmenon happens in the future.\n",
    "\n",
    "3. The ridge regression is more vulnerable to outliers than LS-SVR when the true predictors are unknown. However, if the true predictors are known, the ridge regression is more robust than LS-SVR.\n",
    "\n",
    "4. Weighted least squares methods can improve the fitting of both LS-SVR and ridge regression when the dataset is containminated by outliers. However, compared to weighted LS-SVR, the improvement for weighted ridge regression is not significant. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. References\n",
    "\n",
    "1. Suykens, J. A., Van Gestel, T., & De Brabanter, J. (2002). Least Squares Support Vector Machines. World Scientific.\n",
    "\n",
    "2. Schaal, S., & Atkeson, C. G. (1998). Constructive incremental learning from only local information. Neural computation, 10(8), 2047-2084.\n",
    "\n",
    "3. Suykens, J. A., De Brabanter, J., Lukas, L., & Vandewalle, J. (2002). Weighted least squares support vector machines: robustness and sparse approximation. Neurocomputing, 48(1-4), 85-105."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
